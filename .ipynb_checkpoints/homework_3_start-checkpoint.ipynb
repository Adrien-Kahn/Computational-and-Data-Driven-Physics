{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I6lRgaguLyw"
   },
   "source": [
    "# Homework 3: Text classification with Maximum Entropy model\n",
    "\n",
    "Can we detect sarcasm in news article headlines?\n",
    "\n",
    "In this homework, we will formulate a Maximum Entropy (MaxEnt) models to classify news headlines into sarcastic or not sarcastic.\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019). https://arxiv.org/pdf/1908.07414.pdf\n",
    "2. McCallum, Andrew, and Kamal Nigam. \"A comparison of event models for naive bayes text classification.\" AAAI-98 workshop on learning for text categorization. Vol. 752. No. 1. 1998. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.9324&rep=rep1&type=pdf\n",
    "3. Kaggle dataset: https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection\n",
    "\n",
    "## News article headlines dataset\n",
    "\n",
    "We will use the \"Sarcasm headlines\" dataset, collected by Ref. (1), and downloaded from Kaggle (3). The dataset consists of a series of headlines from news articles, classified as either being sarcastic, or not. The authors of (1) collected news from two sources: a serious news website (https://www.huffingtonpost.com), and an online newspaper that publishes satirical articles (https://www.theonion.com/). Headlines form the The Huffingtonpost were classified as non-sarcastic (`is_sarcastic=0`), and headlines from The Onion were classified as sarcastic (`is_sarcastic=1`).\n",
    "\n",
    "**Data**. Two files have been prepared for this exercise: `train_data.json` containing the dataset that you will use to train the model, and `tests_data.json`, that you will use to evaluate the performance of the model. The following code will download these two data files for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1667289203609,
     "user": {
      "displayName": "Jorge Fernández de Cossío Díaz",
      "userId": "02544861930102826650"
     },
     "user_tz": -60
    },
    "id": "Jm-FyJFsvuhU",
    "outputId": "4b3c04b0-ed92-4f31-fa7e-7ef2c73719f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tests_data.json', <http.client.HTTPMessage at 0x2e091d43730>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the training and test datasets\n",
    "\n",
    "train_url = \"https://gist.github.com/cossio/5fa29aa95d629802e97cf16f8d6c86b9/raw/29106c1e0e230b6f19895914cadce13a9bcb7e1b/train_data.json\"\n",
    "tests_url = \"https://gist.github.com/cossio/5fa29aa95d629802e97cf16f8d6c86b9/raw/29106c1e0e230b6f19895914cadce13a9bcb7e1b/tests_data.json\"\n",
    "\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(train_url, \"train_data.json\")\n",
    "urllib.request.urlretrieve(tests_url, \"tests_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUI9EFA8zg2i"
   },
   "source": [
    "After downloading the data, you can use the following code to load and parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1667289203610,
     "user": {
      "displayName": "Jorge Fernández de Cossío Díaz",
      "userId": "02544861930102826650"
     },
     "user_tz": -60
    },
    "id": "paz_feOvv1hX",
    "outputId": "7d758a74-2f59-4478-bc9c-5233bad57581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "{'is_sarcastic': 0, 'headline': 'sónar festival offers more than you might expect', 'article_link': 'https://www.huffingtonpost.com/entry/sonar-festival-offers-mor_b_5507780.html'}\n",
      "{'is_sarcastic': 1, 'headline': '2013 year in review photo essay shaping up to be quite horrific', 'article_link': 'https://www.theonion.com/2013-year-in-review-photo-essay-shaping-up-to-be-quite-1819575009'}\n",
      "{'is_sarcastic': 1, 'headline': 'angolan war criminal called in as character witness to manafort fraud trial', 'article_link': 'https://politics.theonion.com/angolan-war-criminal-called-in-as-character-witness-to-1828084812'}\n",
      "tests data:\n",
      "{'is_sarcastic': 1, 'headline': \"mother comes pretty close to using word 'streaming' correctly\", 'article_link': 'https://www.theonion.com/mother-comes-pretty-close-to-using-word-streaming-cor-1819575546'}\n",
      "{'is_sarcastic': 1, 'headline': 'shadow government getting too large to meet in marriott conference room b', 'article_link': 'https://politics.theonion.com/shadow-government-getting-too-large-to-meet-in-marriott-1819570731'}\n",
      "{'is_sarcastic': 1, 'headline': 'area boy enters jumping-and-touching-tops-of-doorways phase', 'article_link': 'https://www.theonion.com/area-boy-enters-jumping-and-touching-tops-of-doorways-p-1819570282'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def parse_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for l in file:\n",
    "            yield json.loads(l)\n",
    "\n",
    "train_data = list(parse_data('train_data.json'))\n",
    "tests_data = list(parse_data('tests_data.json'))\n",
    "all_data = train_data + tests_data\n",
    "\n",
    "# Use less train data! (for Question 5)\n",
    "#train_data, tests_data = all_data[:2000], all_data[2000:]\n",
    "\n",
    "# print some examples\n",
    "print('train data:')\n",
    "for d in train_data[:3]:\n",
    "    print(d)\n",
    "print('tests data:')\n",
    "for d in tests_data[:3]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGJ3Beauxmmr"
   },
   "source": [
    "As you can see, each data point is a Python dictionary with three fields:\n",
    "\n",
    "1. `'is_sarcastic'`, a binary value, equal to 1 if the headline is sarcastic, and equal to 0 otherwise.\n",
    "2. `'headline'`, the news article headline.\n",
    "3. `'article_link'`, a link to the original news article. Although in principle inspecting the news article itself can provide more information, in this tutorial we will **not** do this. For simplicity, please ignore the `'article_link'` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1666958148928,
     "user": {
      "displayName": "Jorge Fernández de Cossío Díaz",
      "userId": "02544861930102826650"
     },
     "user_tz": -120
    },
    "id": "ljpIeeIiwYSX",
    "outputId": "d7e19ffb-d967-4718-b31f-c1f326e3b191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "27619\n"
     ]
    }
   ],
   "source": [
    "# number of train and testing data points\n",
    "print(len(train_data))\n",
    "print(len(tests_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1666958311771,
     "user": {
      "displayName": "Jorge Fernández de Cossío Díaz",
      "userId": "02544861930102826650"
     },
     "user_tz": -120
    },
    "id": "E1nmldUBdDNf",
    "outputId": "3d8e5eac-3e61-4a4c-ab16-37925b6e0913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999930116356266"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) / (len(train_data) + len(tests_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1666958149635,
     "user": {
      "displayName": "Jorge Fernández de Cossío Díaz",
      "userId": "02544861930102826650"
     },
     "user_tz": -120
    },
    "id": "g258AHUKxp5o",
    "outputId": "eb372dfe-c7a0-4d82-bbe8-65cc67d66284"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'world will miss goal for universal education by 50 years: un'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access 'headline' of the news article #6 from the training dataset\n",
    "train_data[5]['headline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYrAp-fZ1QKn"
   },
   "source": [
    "To keep things simple, we will ``define'' a word as any portion of text delimited by spaces. The first thing we will do is to construct a list of all words encountered in all the datasets. We will then construct a dictionary giving the index of any word in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666958149990,
     "user": {
      "displayName": "Jorge Fernández de Cossío Díaz",
      "userId": "02544861930102826650"
     },
     "user_tz": -120
    },
    "id": "58Ly1LwlxqNz",
    "outputId": "19e97400-e667-427c-9349-91fd7d0f2791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38234\n"
     ]
    }
   ],
   "source": [
    "# collect all words in a single list (set discards duplicates)\n",
    "all_words = list(set(word for doc in all_data for word in doc['headline'].split()))\n",
    "\n",
    "# total number of words\n",
    "L = len(all_words)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_JEay2y1GEM"
   },
   "outputs": [],
   "source": [
    "# dictionary giving the index of a word in the list\n",
    "word_index = {word: i for (i, word) in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666958150239,
     "user": {
      "displayName": "Jorge Fernández de Cossío Díaz",
      "userId": "02544861930102826650"
     },
     "user_tz": -120
    },
    "id": "E8FRax281i39",
    "outputId": "d2bb9644-261c-47c3-e796-a6a9a45a52a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16161"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index of word 'expertise' in the list of all words\n",
    "word_index['expertise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pC-DC7yJYs4"
   },
   "source": [
    "## Question 1\n",
    "\n",
    "We represent a headline as a binary vector, $\\vec\\sigma=\\{σ_i\\}_{i=1}^L$, where $σ_i = 1$ if word $i$ appears in the headline, and $σ_i = 0$ otherwise. The vector $\\vec\\sigma$ is of length $L$, which equals the total number of words considered (the vocabulary, `all_words` in the code above).\n",
    "\n",
    "**Q1a):** This representation of texts is missing important aspects of language. Can you give a simple argument for why?\n",
    "\n",
    "We will formulate two maximum entropy models (one for each class) matching the empirical frequencies of words in the train dataset for each class. Let $c=0,1$ be our label ($c=1$ for sarcastic headlines, $c=0$ otherwise). For each word, we define its empirical frequency in headlines of class $c$ as follows:\n",
    "\n",
    "$$p_i(c) = \\frac{1}{M_c}\\sum_{\\vec\\sigma\\in\\mathcal D_c}\\sigma_i$$\n",
    "\n",
    "where $\\mathcal D_c$ is the set of headlines of class $c$, and $M_c=|\\mathcal D_c|$ is their number in our dataset. Notice that if a word appears more than once in a headline, it will still be counted as one. Since some rare words occur very few times in the data, it is recommended to add a *pseudo-count*,\n",
    "\n",
    "$$p_i(c) = \\frac{1}{M_c+1}\\left(\\sum_{\\vec\\sigma\\in\\mathcal D_c}\\sigma_i + 1\\right)$$\n",
    "\n",
    "This avoids unintentional divisions by zero below. This procedure will be better justified in the following lectures when we discuss the role of the prior distributtion.\n",
    "\n",
    "As shown in the lecture, the MaxEnt model matching these empirical frequencies for each class, is of the form:\n",
    "\n",
    "$$P(\\vec{\\sigma}|c) = \\frac{e^{\\sum_i h_i(c)\\sigma_i}}{Z(c)}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$Z (c) = \\sum_{\\vec{\\sigma}}e^{\\sum_i h_i (c) \\sigma_i} = \\prod_i\n",
    "\\sum_{\\sigma=0,1}e^{h_i (c) \\sigma_i} = \\prod_i (1 + e^{h_i (c)})$$\n",
    "\n",
    "and the fields $h_i(c)$ are such that the moment-matching constraints are satisfied.\n",
    "\n",
    "**Q1b)** Write the expression of the fields $h_i(c)$ to satisfy the moment-matching constraints $\\langle \\sigma_i \\rangle_c = p_i(c)$, where $\\langle . \\rangle_c$ indicates the expectations under the model.\n",
    "\n",
    "**Q1c)** Estimate the fields $h_i(c)$ for all the words in the full dataset, but using only the training data to estimate the empirical frequencies $p_i(c)$ (use the pseudocount, as explained above). Explain why the pseudo-count is necessary. *Hint: Consider a word that occurs in the testset, but not in the training set.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXAEmrpIJfZm"
   },
   "source": [
    "## Question 2.\n",
    "\n",
    "**Q2a)** Plot the histogram of $\\log P(\\vec\\sigma|c=1)$ for the sarcastic headlines, and for the serious headlines (in the same plot), for the test dataset. Are the histograms different?\n",
    "\n",
    "**Q2b)** Plot the histogram of $\\log\\frac{P(\\vec\\sigma|c=1)}{P(\\vec\\sigma|c=0)}$ for the sarcastic headlines, and for the serious headlines. Are the histograms different?\n",
    "\n",
    "**Bonus:** Same as b), but for $\\log P(\\vec\\sigma|c=0)$.\n",
    "\n",
    "**Q2c)** To give a quantitative measure of the separation of the two histograms above, you can compute the difference in means, normalized by the geometric mean of their standard deviations:\n",
    "\n",
    "$$\\frac{|\\langle x\\rangle - \\langle y\\rangle|}{\\sqrt{\\sigma_x \\sigma_y}}$$\n",
    "\n",
    "where $x,y$ denote the two kind of log-likelihoods or log-likelihood differences, considered in a), b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xAJCecv7Nsn"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "**Q3a)** Using Bayes theorem, compute the ratio $P(c=1|\\vec{\\sigma})/P(c=0|\\vec{\\sigma})$ for the headlines in your testing set (note that $P(\\vec{\\sigma})$ cancels). To obtain an empirical estimate of $P(c)$, you can count the number of headlines in each class of the training set. Your classifier will use these ratios as the scores to predict the label of a headline.\n",
    "\n",
    "**Q3b)** How accurate is this classifier? To quantify this, plot the Receiver Operating Characteristic Curve (ROC) and compute the Area Under the Curve (AUC). See Appendix for an explanation of the ROC and the AUC. *Hint: The ROC curve is easier to obtain if you assume that the scores are sorted. You can also use the functions `roc_curve` and `auc` from the python package `sklearn`.*\n",
    "\n",
    "**Bonus:** What is the ROC curve and AUC of a random classifier (that emits random labels for each headline)? What is the ROC curve and AUC of a perfect classifier (that emits the correct label for every headline)?\n",
    "\n",
    "**Bonus:** Describe the connection between the areas under the histogram  curves of exercise 2 from d) and the AUC.\n",
    "\n",
    "**(BONUS) Q3c)** Estimate the mutual information between the headline and the label,\n",
    "\n",
    "$$MI(\\vec\\sigma,c)=\\sum_{\\vec\\sigma,c}P(\\vec\\sigma,c)\\ln\\left(\\frac{P(\\vec\\sigma,c)}{P(\\vec\\sigma)P(c)}\\right)$$\n",
    "\n",
    "Using the exact expression is intractable. However, you can obtain a practical estimate by approximating the outer average over the model distribution $P(\\vec\\sigma,c)$, with an empirical average over your data,\n",
    "\n",
    "$$MI(\\vec\\sigma,c)\\approx\\frac{1}{M}\\sum_d\\ln\\left(\\frac{P(\\vec\\sigma_d,c_d)}{P(\\vec\\sigma_d)P(c_d)}\\right)$$\n",
    "\n",
    "where the sum over $d$ goes over the labeled headlines $\\vec\\sigma_d,c_d$ of your dataset. Estimate $MI(\\vec\\sigma,c)$ using this approximate expression for your training set and your testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYbVCUcm1y_j"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Compute the Kullback-Leiber (KL) divergences between the 2 models,\n",
    "\n",
    "$$D_{KL}(P(\\vec\\sigma|c=0) || P(\\vec\\sigma|c=1))$$\n",
    "\n",
    "and\n",
    "\n",
    "$$D_{KL}(P(\\vec\\sigma|c=1) || P(\\vec\\sigma|c=0))$$\n",
    "\n",
    "Why are the two KL divergences different?\n",
    "\n",
    "**Q4a)** Show that the KL divergences above, are sums of contributions for each word. Derive analytical expressions, as functions of the fields $h_i(c)$ in each case.\n",
    "\n",
    "**Q4b)** Consider the task of classifying a newspaper as publishing sarcastic or non-sarcastic news articles. Estimate (approximatey) how many headlines your model will need to guess the correct classification confidently (with a probability of error below $10^{-10}$), assuming that in reality, i) the news-source publishes sarcastic headlines, or ii) the news-source publishes serious articles.\n",
    "\n",
    "**Q4c)** To model different newspapers, construct datasets $D_1(c=1), D_2(c=1), \\dots, D_{10}(c=1)$ consisting of different numbers of sarcastic headlines only, by taking the first 5, 10, 15, ..., 50, sarcastic headlines from the test dataset. Similarly, construct datasets $D_1(c=0), D_2(c=0), \\dots, D_{10}(c=0)$ consisting of serious headlines only, by taking the first 5, 10, 15, .. 50 serious headlines from the test dataset. Compute the log-likelihood of each of these datasets under the two models $P(\\vec\\sigma|c=0,1)$.\n",
    "\n",
    "**Q4d)** Plot the log-likelihood difference versus the number of headlines in each of the datasets constructed in Q4c). What is the expected slope of this plot?\n",
    "\n",
    "**Q4e)** Plot the log-likelihood difference per headlines (i.e., divided by number of headlines), versus the number of headlines in each of the datasets constructed in c). What is the expected asymptotic of this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo9448SBdAOQ"
   },
   "source": [
    "## Question 5\n",
    "\n",
    "In the previous exercises, your training dataset contained $\\approx 80\\%$ of headlines in your entire dataset. Repeat exericses 1-4, but reducing your training dataset to only the first 100, 1000, and 10000 headlines, assigning all the remaining headlines to the testing set. Discuss the impact on the performance of the model of the limited training data. Plot the AUC for the different sizes of training data, and the score computed in Q2c. *Hint: You can just modify the definition of `train_data` in the first cell of this notebook and re-run everything else!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eUuuaZ98FA1"
   },
   "source": [
    "# Appendix: ROC and AUC\n",
    "\n",
    "Giving predicted scores $S_i$ (in the exercises above you will use the odds ratio $P(c=1|\\vec\\sigma)/P(c=0|\\vec\\sigma)$ of the headlines $\\vec\\sigma_i$, and their true labels, $y_i=0,1$, we consider a threshold $t$ and assume that headlines for which $S_i > t$ are predicted to be sarcastic, while those for which $S_i\\le t$ are predicted to be non-sarcastic. We define:\n",
    "\n",
    "$$TP(t) = \\text{number of true positives}\\quad\n",
    "(S_i > t\\, \\text{and}\\, y_i=1)$$\n",
    "$$FP(t) = \\text{number of false positives}\\quad\n",
    "(S_i > t\\, \\text{and}\\, y_i=0)$$\n",
    "\n",
    "That is, $TP(t)$ is the number of correctly classified positive (sarcastic) headlines, and $FP(t)$ is the number of wrongly classified negative (non-sarcastic) headlines. In addition, let $P$ be the total number of sarcastic headlines in the data, and $N$ the total number of non-sarcastic headlines. Finally we define:\n",
    "\n",
    "$$TPR(t) = \\frac{TP(t)}{P} ,\\qquad FPR(t) = \\frac{FP(t)}{N}$$\n",
    "\n",
    "The ROC curve is defined as the parametric curve $(FPR(t), TPR(t))$, traversed as the threshold takes all possible values, $-\\infty<t<\\infty$. Notice that $0\\le TPR(t)\\le1$ and $0\\le FPR(t)\\le1$, and that:\n",
    "\n",
    "$$TPR(-\\infty)=FPR(-\\infty)=1,\\qquad TPR(\\infty)=FPR(\\infty)=0$$.\n",
    "\n",
    "The AUC is defined as the area under this curve:\n",
    "\n",
    "$$AUC = \\int_{t=-\\infty}^\\infty TPR(t) \\times \\mathrm d FPR(t)\n",
    "= \\int_{t=-\\infty}^\\infty TPR(t) \\times \\frac{\\mathrm d FPR(t)}{\\mathrm d t} \\mathrm{d}t$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNpBRtaQLgECQLyas7ab/N",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1AiY0ZuY1q7crbK6_MDv4R1f0PCdI8a08",
     "timestamp": 1666767222246
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
